# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18GJCoOXRytvOqS9htUHuqv5jHP7BTbwd
"""

!pip install requests_html
!pip install pyngrok
!pip install flask_ngrok

from flask import Flask, render_template
from flask_ngrok import run_with_ngrok
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.cluster import KMeans

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

!pip install pygooglenews --upgrade

from pygooglenews import GoogleNews

gn = GoogleNews()

articles = []
search = gn.geo_headlines('Zimbabwe')
newsItems = search['entries']
for item in newsItems:
  story = {
      'title':item.title,
      'link': item.link,
      'source': item.source.title,
  }
  articles.append(story)

articles

!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo "deb https://ngrok-agent.s3.amazonaws.com buster main" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok

!ngrok authtoken 2MeHdcAHY3HoTkafvVWgWnFmWHC_4cgi9pEZfyaSPBUCcZW5d

app = Flask(__name__)
run_with_ngrok(app)

# Load the dataset
df = pd.DataFrame.from_records(articles)

# Data cleaning
df['title'] = df['title'].str.lower()
df['title'] = df['title'].str.replace('[^\w\s]','') 
df['title'] = df['title'].str.replace('\d+', '')
df['title'] = df['title'].str.replace('\n', '')
df['title'] = df['title'].str.strip()

# Tokenization
df['title'] = df['title'].apply(nltk.word_tokenize)

# Stop word removal
stop_words = set(stopwords.words('english'))
df['title'] = df['title'].apply(lambda x: [item for item in x if item not in stop_words])

# Stemming and Lemmatization
ps = PorterStemmer()
lemmatizer = WordNetLemmatizer()
df['title'] = df['title'].apply(lambda x: [ps.stem(word) for word in x])
df['title'] = df['title'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])

# Join tokens to form string
df['title'] = df['title'].apply(lambda x: ' '.join(x))

# Vectorization
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['title'])

# Clustering
kmeans = KMeans(n_clusters=5)
kmeans.fit(X)

# Define a route for the homepage
@app.route('/')
def index():
    # Get the clusters and their titles
    clusters = []
    for i in range(5):
        cluster = np.where(kmeans.labels_ == i)[0]
        cluster_titles = [df.iloc[j]["title"] for j in cluster]
        clusters.append(cluster_titles)
    return render_template('index.html', clusters=clusters)

if __name__ == '__main__':
    app.run()



